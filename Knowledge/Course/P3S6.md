Alright, let's move into **Phase 3, Step 6: Asynchronous Background Tasks & Task Queues**. This is a crucial topic for building truly scalable and responsive backend applications.

-----

### Phase 3, Step 6: Asynchronous Background Tasks & Task Queues

#### 1\. Problem Statement: Why Background Tasks?

In a typical web API, when a client makes a request, the server processes it and sends a response. If an operation takes a long time (e.g., more than a few hundred milliseconds or a second), it can lead to:

  * **Blocked API Responses:** The client has to wait for the entire long-running operation to complete before receiving a response, leading to a slow and unresponsive user experience.
  * **API Timeouts:** Load balancers or proxies might terminate connections if the API doesn't respond within a set timeout.
  * **Resource Hogging:** While one request is busy with a long-running task, the server might not be able to handle other incoming requests efficiently, impacting overall system throughput.
  * **Failure During Response:** If a long operation fails *after* sending an initial success signal, the client might not know about it.

**Examples of Long-Running Operations:**

  * Processing image uploads (resizing, applying filters).
  * Generating complex reports (PDFs, Excel files).
  * Sending bulk emails or notifications.
  * Data migrations or batch processing.
  * Calling slow third-party APIs.
  * Machine learning model inference.

**Solution:** Offload these operations to a **background task queue**.

#### 2\. Solutions: Task Queues

A task queue system involves:

  * **Producer/Client (your FastAPI app):** Dispatches (sends) tasks to the queue. It typically gets an immediate response confirming the task was queued.
  * **Broker (Message Queue):** A message broker (like Redis or RabbitMQ) acts as a central hub where tasks are temporarily stored. It ensures reliable delivery to workers.
  * **Consumer/Worker (Celery worker):** Long-running processes that continuously listen to the broker, pick up tasks, and execute them.

This setup decouples the web server from the heavy lifting, allowing the API to respond quickly and the background tasks to run independently.

**Popular Python Task Queue Libraries:**

  * **Celery:** The most widely used and feature-rich distributed task queue for Python. Supports various brokers (RabbitMQ, Redis, Amazon SQS, etc.). Excellent for complex workflows, retries, scheduling, etc.
  * **Redis Queue (RQ):** A simpler, Redis-only task queue. Good for smaller-scale projects or when you specifically want Redis as your broker.
  * **Huey:** Another lightweight, Redis-backed task queue.

We will focus on **Celery** as it's the industry standard for production-grade Python backends. We'll use **Redis** as our message broker due to its simplicity for development.

-----

#### 3\. Implementing with Celery

**a. Pre-requisites:**

You need a running Redis server.

  * **Docker (Recommended):** `docker run --name my-redis -p 6379:6379 -d redis/redis-stack-server` (or just `redis`)
  * **Direct Install:** Follow instructions for your OS (e.g., `sudo apt-get install redis-server` on Ubuntu, `brew install redis` on macOS).

**b. Installations:**

```bash
# Ensure your virtual environment is activated
pip install celery redis
```

**c. Basic Celery Setup (`celery_worker.py`):**

This file will define your Celery application and the tasks it will execute.

```python
# celery_worker.py
from celery import Celery
import time
import smtplib # For simulating email sending
from email.message import EmailMessage

# Celery application instance
# broker: URL of the message broker (Redis in this case)
# backend: URL for storing task results (also Redis here, could be database)
celery_app = Celery(
    'my_app', # Name of the Celery app
    broker='redis://localhost:6379/0', # Redis broker on default port, database 0
    backend='redis://localhost:6379/1'  # Redis backend for results on database 1
)

# Optional: Configuration can be loaded from a separate file or dict
# celery_app.conf.update(
#     task_serializer='json',
#     accept_content=['json'],
#     result_serializer='json',
#     timezone='UTC',
#     enable_utc=True,
# )

# --- Define Celery Tasks ---

@celery_app.task(name="tasks.send_welcome_email") # 'name' is optional, but good for explicit naming
def send_welcome_email(recipient_email: str, username: str):
    """Simulates sending a welcome email."""
    print(f"[{celery_app.current_task.request.id}] Sending welcome email to {recipient_email} for user {username}...")
    time.sleep(5) # Simulate network delay for sending email
    # In a real app, integrate with an email sending service (SendGrid, Mailgun, etc.)
    # For demonstration, let's just print a message.

    # msg = EmailMessage()
    # msg.set_content(f"Welcome to our service, {username}!")
    # msg['Subject'] = 'Welcome to Our App!'
    # msg['From'] = 'no-reply@yourapp.com'
    # msg['To'] = recipient_email
    #
    # try:
    #     with smtplib.SMTP('smtp.mailtrap.io', 2525) as smtp: # Example using Mailtrap for testing
    #         smtp.starttls()
    #         smtp.login('your_mailtrap_username', 'your_mailtrap_password')
    #         smtp.send_message(msg)
    #     print(f"Email sent successfully to {recipient_email}")
    # except Exception as e:
    #     print(f"Failed to send email to {recipient_email}: {e}")

    print(f"[{celery_app.current_task.request.id}] Welcome email sent to {recipient_email}.")
    return f"Email to {recipient_email} processed."

@celery_app.task(name="tasks.generate_report")
def generate_report(user_id: int):
    """Simulates generating a complex report for a user."""
    print(f"[{celery_app.current_task.request.id}] Generating report for user {user_id}...")
    time.sleep(10) # Simulate long report generation
    report_data = f"Report for user {user_id}: Generated at {time.ctime()}"
    print(f"[{celery_app.current_task.request.id}] Report for user {user_id} generated.")
    return report_data

@celery_app.task(bind=True, name="tasks.long_running_calc") # bind=True allows accessing task instance
def long_running_calculation(self, a: int, b: int):
    """A task that might update its progress."""
    print(f"[{self.request.id}] Starting long calculation: {a} + {b}...")
    for i in range(1, 6):
        time.sleep(2)
        progress = i * 20
        self.update_state(state='PROGRESS', meta={'progress': progress, 'current': i, 'total': 5})
        print(f"[{self.request.id}] Progress: {progress}%")
    result = a + b
    print(f"[{self.request.id}] Calculation finished: {result}")
    return result
```

**d. Running a Celery Worker:**

1.  Open a new terminal window.

2.  Navigate to the directory containing `celery_worker.py`.

3.  Ensure your virtual environment is activated.

4.  Run:

    ```bash
    celery -A celery_worker worker --loglevel=info
    ```

      * `celery -A celery_worker`: Tells Celery to load the application from `celery_worker.py`.
      * `worker`: Specifies that you want to run a worker process.
      * `--loglevel=info`: Sets the logging level.

    You should see output indicating the worker has started and is connected to Redis. Keep this terminal open and running.

**e. Calling a Task from FastAPI (`main.py`):**

Now, let's modify our `main.py` (or `app.py`) to dispatch tasks.

```python
# main.py (excerpt, assuming you have the FastAPI setup from previous steps)
# ... imports from fastapi, sqlalchemy, schemas, crud, auth, etc.
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
import uvicorn
from typing import Annotated

# Import the Celery app instance from celery_worker.py
from .celery_worker import celery_app, send_welcome_email, generate_report, long_running_calculation

# ... (database setup, get_db, get_current_user, get_current_active_user dependencies) ...

app = FastAPI(title="FastAPI + Celery Demo")

# ... (Authentication endpoints: /register, /token, /users/me/) ...
# ... (User CRUD endpoints: /users/, /users/{user_id}) ...
# ... (Item CRUD endpoints: /users/{user_id}/items/, /items/, /items/{item_id}) ...


# --- NEW: Endpoints to Dispatch Celery Tasks ---

@app.post("/tasks/send-email/", status_code=status.HTTP_202_ACCEPTED, summary="Dispatch email sending task")
async def dispatch_send_email(
    recipient_email: str,
    username: str,
    current_user: Annotated[models.User, Depends(get_current_active_user)] # Requires authentication
):
    """
    Dispatches a task to send a welcome email to the specified recipient.
    Returns task ID for tracking.
    """
    # .delay() is a shortcut for .apply_async(args=args, kwargs=kwargs)
    task = send_welcome_email.delay(recipient_email, username)
    return {"message": "Email sending task dispatched", "task_id": task.id}

@app.post("/tasks/generate-report/", status_code=status.HTTP_202_ACCEPTED, summary="Dispatch report generation task")
async def dispatch_generate_report(
    user_id: int,
    current_user: Annotated[models.User, Depends(get_current_active_user)]
):
    """
    Dispatches a task to generate a report for a user.
    Returns task ID for tracking.
    """
    task = generate_report.delay(user_id)
    return {"message": f"Report generation for user {user_id} dispatched", "task_id": task.id}

@app.post("/tasks/long-calc/", status_code=status.HTTP_202_ACCEPTED, summary="Dispatch long calculation task")
async def dispatch_long_calc(
    a: int,
    b: int,
    current_user: Annotated[models.User, Depends(get_current_active_user)]
):
    """
    Dispatches a long-running calculation task.
    """
    task = long_running_calculation.delay(a, b)
    return {"message": f"Long calculation task for {a}+{b} dispatched", "task_id": task.id}


# --- NEW: Endpoint to Check Task Status ---

@app.get("/tasks/{task_id}/status", summary="Get status of a background task")
async def get_task_status(task_id: str):
    task = celery_app.AsyncResult(task_id) # Get the AsyncResult object for the task ID

    if task.state == 'PENDING':
        response = {
            'state': task.state,
            'status': 'Task is pending or not found'
        }
    elif task.state == 'PROGRESS':
        response = {
            'state': task.state,
            'status': 'Task is in progress',
            'meta': task.info.get('progress', 0) # Get progress from meta if available
        }
    elif task.state != 'FAILURE':
        response = {
            'state': task.state,
            'result': task.result # Get the result if task succeeded
        }
    else:
        # Task failed
        response = {
            'state': task.state,
            'status': str(task.info),  # Exception information
        }
    return response


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
```

**Project Structure:**

```
my_fastapi_app/
├── main.py
├── database.py
├── models.py
├── schemas.py
├── crud.py
├── auth.py
└── celery_worker.py  <-- NEW FILE
```

**How to Run and Test:**

1.  **Start Redis:** `docker run --name my-redis -p 6379:6379 -d redis/redis-stack-server`
2.  **Run Migrations:** If you changed models, ensure you run `Base.metadata.create_all(bind=engine)` from `database.py` again.
3.  **Start FastAPI:** In one terminal: `uvicorn main:app --reload`
4.  **Start Celery Worker:** In a *separate* terminal: `celery -A celery_worker worker --loglevel=info`
5.  **Test:**
      * **Register a user and get a token** (as in previous step).
      * **Dispatch an email task:**
          * `POST http://127.0.0.1:8000/tasks/send-email/`
          * Headers: `Authorization: Bearer <your_token>`
          * Query Params (or JSON body): `recipient_email=test@example.com`, `username=TestUser`
          * Expected: `202 Accepted` with `{"message": "...", "task_id": "..."}`
          * **Observe the Celery worker terminal:** You should see the `Sending welcome email...` log after a moment.
      * **Check task status:**
          * `GET http://127.0.0.1:8000/tasks/<task_id_from_above>/status`
          * You'll see `PENDING`, then `PROGRESS`, then `SUCCESS` (or `FAILURE` if there's an issue) and the result.
      * **Dispatch a report generation task:**
          * `POST http://127.0.0.1:8000/tasks/generate-report/`
          * Headers: `Authorization: Bearer <your_token>`
          * Query Params: `user_id=1` (use an existing user ID)
          * Observe the worker taking 10 seconds.
      * **Dispatch a long calculation task:**
          * `POST http://127.0.0.1:8000/tasks/long-calc/`
          * Headers: `Authorization: Bearer <your_token>`
          * Query Params: `a=100`, `b=200`
          * Observe the progress updates in the worker and check the status endpoint multiple times.

**f. Error Handling in Tasks:**

  * Tasks can `raise` exceptions. Celery will mark the task as `FAILURE`, and the traceback will be stored in the result backend.
  * You can configure automatic retries for transient failures.

**g. Monitoring (Flower):**

  * For a production setup, a monitoring tool like **Flower** is invaluable.
  * Installation: `pip install flower`
  * Run: `celery -A celery_worker flower --port=5555`
  * Access in browser: `http://localhost:5555`
  * Flower provides a real-time web interface to monitor tasks, workers, and results.

-----

#### Your Turn to Practice:

1.  **Integrate Celery into Your Task Manager API:**
      * Take your existing `fastapi_auth_tasks` project.
      * Set up `celery_worker.py` as demonstrated.
      * **Refactor `create_task`:** When a `POST /tasks` request comes in, instead of immediately processing it:
          * Dispatch a Celery task (e.g., `process_new_task`) to do the actual database insertion.
          * The API endpoint should return a `202 Accepted` status code immediately, along with the `task_id` for tracking.
      * **Add a "send reminder" task:** Create a new Celery task `send_task_reminder_email(task_id, user_email)` that simulates sending a reminder. Create an API endpoint (e.g., `POST /tasks/{task_id}/remind`) that dispatches this task.
      * **Implement task status checking:** Add an endpoint `GET /tasks/background/{task_id}/status` similar to the example to check the status of `process_new_task` or `send_task_reminder_email`.
      * Run Redis, your FastAPI app, and your Celery worker concurrently. Test the flow end-to-end.

This exercise will solidify your understanding of how to use background tasks to keep your API responsive while handling complex or time-consuming operations asynchronously.

When you're ready, we'll move on to **Phase 3, Step 7: Deployment & Containerization (Docker)**. This will bring everything together to prepare your application for a real-world environment.
Alright, let's make your application observable\! We're now moving to **Phase 4, Step 2: Monitoring, Logging, & Alerting**.

This phase is about understanding the health, performance, and behavior of your API in production. You can't fix what you don't see.

-----

### Phase 4, Step 2: Monitoring, Logging, & Alerting

Observability is paramount for production systems. It allows you to:

  * **Troubleshoot Issues:** Quickly diagnose the root cause of problems.
  * **Understand Performance:** Identify bottlenecks and optimize resource usage.
  * **Ensure Security:** Detect suspicious activities.
  * **Gain Business Insights:** Understand user behavior and application usage.
  * **Proactively Detect Problems:** Be alerted *before* users report issues.

It typically encompasses three pillars: **Logs, Metrics, and Traces**.

#### 1\. Logging

Logs are timestamped records of events that occur within your application. They provide a narrative of what happened.

  * **Python's Built-in `logging` Module:** Python has a powerful and flexible logging module.

      * **Levels:**
          * `DEBUG`: Detailed information, typically only of interest when diagnosing problems.
          * `INFO`: Confirmation that things are working as expected.
          * `WARNING`: An indication that something unexpected happened, or indicative of some problem in the near future (e.g., 'disk space low'). The software is still working as expected.
          * `ERROR`: Due to a more serious problem, the software has not been able to perform some function.
          * `CRITICAL`: A serious error, indicating that the program itself may be unable to continue running.
      * **Configuration:** You can configure handlers (where logs go, e.g., console, file, syslog), formatters (how logs look), and filters.

  * **Best Practices:**

      * **Structured Logging (JSON Logs):** Instead of free-form text, log data as structured JSON objects. This makes logs easily parsable by machines, queryable, and analyzable by logging tools.
          * Example: `{"timestamp": "...", "level": "INFO", "message": "User logged in", "user_id": 123, "ip_address": "192.168.1.1"}`
      * **Contextual Logging:** Include relevant context (e.g., `user_id`, `request_id`, `transaction_id`) in your log messages. This helps trace events related to a specific user or request across different parts of your application.
      * **Appropriate Logging Levels:** Don't log everything at `ERROR`. Use levels correctly to distinguish severity.
      * **Centralized Logging:** For production, logs should be aggregated from all your services into a central logging system.
          * **Tools:**
              * **ELK Stack:** Elasticsearch (storage/search), Logstash (ingestion/processing), Kibana (visualization).
              * **Loki + Grafana:** Loki is a log aggregation system, designed for cost-effectiveness and scalability, especially when paired with Grafana for visualization.
              * **Cloud-Native Solutions:** AWS CloudWatch Logs, Google Cloud Logging, Azure Monitor Logs.

**FastAPI Implementation Example (Structured Logging):**

We'll use `python-json-logger` for structured JSON output.

1.  **Install:** `pip install python-json-logger`

2.  **`my_fastapi_app/logging_config.py`:**

    ```python
    # my_fastapi_app/logging_config.py
    import logging
    from pythonjsonlogger import jsonlogger

    def setup_logging():
        logger = logging.getLogger() # Get the root logger
        logger.setLevel(logging.INFO) # Set default level

        # Clear existing handlers to prevent duplicate logs if called multiple times
        if logger.hasHandlers():
            logger.handlers.clear()

        logHandler = logging.StreamHandler() # Output to console
        formatter = jsonlogger.JsonFormatter(
            '%(asctime)s %(levelname)s %(name)s %(message)s %(request_id)s %(user_id)s'
            # You can customize the fields to include. %(message)s will be the actual log message.
            # Custom fields like request_id and user_id will be added via extra dict.
        )
        logHandler.setFormatter(formatter)
        logger.addHandler(logHandler)

        # Optional: Set higher level for uvicorn and sqlalchemy to reduce verbosity
        logging.getLogger("uvicorn").setLevel(logging.WARNING)
        logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
        logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)

    # Custom Log Filter to add context like request_id
    class RequestIdFilter(logging.Filter):
        def filter(self, record):
            # This is a simplified example. In a real app, you'd get request_id
            # from a thread-local storage or a context variable set by middleware.
            record.request_id = getattr(record, 'request_id', 'N/A')
            record.user_id = getattr(record, 'user_id', 'N/A')
            return True

    # Add the filter to the root logger
    logging.getLogger().addFilter(RequestIdFilter())
    ```

3.  **`my_fastapi_app/main.py` (integrate logging):**

    ```python
    # my_fastapi_app/main.py
    import logging
    from fastapi import FastAPI, Request, Response, status
    from fastapi.routing import APIRoute # For custom route class
    import time

    from .logging_config import setup_logging # Import setup function

    # --- Setup Logging on App Startup ---
    setup_logging()
    logger = logging.getLogger(__name__) # Get a logger for this module

    # --- Custom APIRoute to add request_id to logs (simplified) ---
    class ContextualRoute(APIRoute):
        def get_route_handler(self):
            original_route_handler = super().get_route_handler()

            async def custom_route_handler(request: Request) -> Response:
                request_id = str(uuid.uuid4()) # Generate a unique ID for each request
                # Attach request_id to the request object (can be accessed later)
                request.state.request_id = request_id

                # Temporarily add request_id to the logger's extra context for this request
                # This is one way; a better way is contextvars for async.
                # For this demo, let's modify the logger directly (be careful with this in complex async setups)
                # A more robust solution uses `contextvars` or FastAPI middleware that wraps the actual handler.
                # For simplicity here, let's just make the logger aware if we pass it later.

                start_time = time.time()
                try:
                    logger.info("Request started", extra={"request_id": request_id, "method": request.method, "path": request.url.path})
                    response = await original_route_handler(request)
                except Exception as e:
                    logger.error(f"Request failed: {e}", extra={"request_id": request_id, "method": request.method, "path": request.url.path}, exc_info=True)
                    raise # Re-raise to let FastAPI's exception handlers take over
                finally:
                    process_time = time.time() - start_time
                    logger.info("Request finished", extra={"request_id": request_id, "method": request.method, "path": request.url.path, "status_code": response.status_code, "process_time_ms": process_time * 1000})
                return response
            return custom_route_handler

    app = FastAPI(
        title="FastAPI MLO Demo",
        routes=[
            ContextualRoute(path=route.path, methods=route.methods, endpoint=route.endpoint)
            for route in app.routes
        ]
    )

    # --- Example of using the logger with custom fields ---
    @app.post("/users/", response_model=schemas.User, status_code=status.HTTP_201_CREATED, summary="Register a new user")
    async def register_user(user: schemas.UserCreate, db: Session = Depends(get_db), request: Request = None):
        logger.info("Attempting to register new user", extra={"request_id": request.state.request_id, "user_email": user.email})
        db_user = crud.get_user_by_email(db, email=user.email)
        if db_user:
            logger.warning("User registration failed: Email already registered", extra={"request_id": request.state.request_id, "user_email": user.email})
            raise HTTPException(status_code=400, detail="Email already registered")
        created_user = crud.create_user(db=db, email=user.email, password=user.password)
        logger.info("User registered successfully", extra={"request_id": request.state.request_id, "user_id": created_user.id, "user_email": created_user.email})
        return created_user

    # ... (other endpoints) ...
    ```

      * **Note on `ContextualRoute`:** This is a simplified way to inject `request_id` into logs. For truly robust async contextual logging, you'd typically use `contextvars` (Python 3.7+) or a dedicated `Request` object in FastAPI's middleware. `starlette_context` or similar libraries can simplify this.

#### 2\. Monitoring (Metrics)

Metrics are numerical measurements collected over time, providing insights into system performance and health.

  * **Key Metrics to Monitor:**
      * **Application Metrics:** Request rates (RPS), error rates, request latency (p99, p95, average), throughput, number of active connections.
      * **System Metrics:** CPU utilization, memory usage, disk I/O, network I/O.
      * **Database Metrics:** Query latency, connection pool size, number of open connections, slow queries.
      * **Celery Metrics:** Task success/failure rates, task queue size, worker health.
  * **Tools:**
      * **Prometheus:** A powerful open-source monitoring system that collects and stores metrics as time series data. It "pulls" (scrapes) metrics from configured targets.
      * **Grafana:** A leading open-source platform for visualizing metrics. It connects to various data sources (like Prometheus) and allows you to create interactive dashboards.

**FastAPI Implementation Example (`fastapi-prometheus`):**

This library provides an easy way to expose Prometheus metrics from your FastAPI application.

1.  **Install:** `pip install fastapi-prometheus prometheus_client`

2.  **`my_fastapi_app/main.py` (integrate metrics):**

    ```python
    # my_fastapi_app/main.py
    from prometheus_fastapi_instrumentator import Instrumentator # Import the instrumentator
    # ... (other imports) ...

    app = FastAPI(
        title="FastAPI MLO Demo",
        # route_class=ContextualRoute # If you're using the custom route for logging context
    )

    # --- Prometheus Instrumentation ---
    @app.on_event("startup")
    async def startup_event():
        setup_logging() # Ensure logging is setup first
        # Connect to Redis for FastAPILimiter
        redis_connection_string = os.getenv("CELERY_BROKER_URL", "redis://redis:6379/0")
        redis = aioredis.from_url(redis_connection_string, encoding="utf-8", decode_responses=True)
        await FastAPILimiter.init(redis)

        # Instrument the FastAPI app for Prometheus metrics
        Instrumentator().instrument(app).expose(app, include_in_schema=False)
        logger.info("FastAPI app started and instrumented for Prometheus.")

    # ... (rest of your FastAPI routes and definitions) ...
    ```

      * This will expose a `/metrics` endpoint (by default) on your FastAPI app. Prometheus will then be configured to scrape this endpoint.
      * You can customize which metrics are exposed, add custom metrics, etc.

**Docker Compose for Prometheus & Grafana:**

To truly see these metrics in action, you'd add Prometheus and Grafana to your `docker-compose.yml`.

```yaml
# my_fastapi_app/docker-compose.yml (excerpt)
version: '3.8'

services:
  # ... (app, worker, db, redis services from previous step) ...

  # Prometheus Service
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus/:/etc/prometheus/ # Mount Prometheus config directory
      - prometheus_data:/prometheus # Persist Prometheus data
    command: --config.file=/etc/prometheus/prometheus.yml --web.enable-remote-write-receiver
    ports:
      - "9090:9090" # Prometheus UI
    depends_on:
      - app # Prometheus needs to scrape app metrics

  # Grafana Service
  grafana:
    image: grafana/grafana:latest
    volumes:
      - grafana_data:/var/lib/grafana # Persist Grafana data
    ports:
      - "3000:3000" # Grafana UI
    depends_on:
      - prometheus # Grafana uses Prometheus as a data source

volumes:
  # ... (postgres_data, redis_data) ...
  prometheus_data:
  grafana_data:
```

**`my_fastapi_app/prometheus/prometheus.yml` (Configuration for Prometheus):**

```yaml
# my_fastapi_app/prometheus/prometheus.yml
global:
  scrape_interval: 15s # How frequently Prometheus scrapes targets

scrape_configs:
  - job_name: 'fastapi_app'
    # metrics_path: '/metrics' # Default, so often not needed
    static_configs:
      - targets: ['app:8000'] # 'app' is the hostname of our FastAPI service in docker-compose
```

After adding these to `docker-compose.yml` and creating the `prometheus` directory with `prometheus.yml`, run `docker compose up --build -d`.

  * Access Prometheus UI: `http://localhost:9090`
  * Access Grafana UI: `http://localhost:3000` (default login admin/admin, change password)
      * In Grafana, add Prometheus as a data source.
      * Then, you can create dashboards to visualize your FastAPI metrics.

#### 3\. Tracing

Tracing tracks the end-to-end flow of a single request or transaction as it propagates through multiple services in a distributed system.

  * **Problem:** In microservices architectures, a single user request might involve dozens of services. If a request is slow or fails, it's hard to pinpoint where the problem occurred.
  * **Solution:** Distributed tracing assigns a unique `trace_id` to each request at its entry point. This `trace_id` is then propagated to all downstream services involved in processing that request. Each service generates `span_id`s for its operations.
  * **Tools:**
      * **OpenTelemetry:** A vendor-neutral set of APIs, SDKs, and tools for instrumenting, generating, collecting, and exporting telemetry data (metrics, logs, and traces). It's the modern standard.
      * **Jaeger:** An open-source distributed tracing system.
      * **Zipkin:** Another open-source distributed tracing system.

**Conceptual Implementation (Requires more setup than can be fully covered here):**

1.  **Instrument your app with OpenTelemetry SDK:** Add middleware or decorators to automatically generate spans for requests and database calls.
2.  **Propagate `trace_id`:** Ensure the `trace_id` is passed in HTTP headers to other services (e.g., to Celery tasks if they interact with the API, or to other microservices).
3.  **Set up an OpenTelemetry Collector:** To receive and process trace data.
4.  **Run a Tracing Backend (Jaeger/Zipkin):** To store and visualize traces.

This is a more advanced topic, but crucial for debugging complex distributed systems.

#### 4\. Alerting

Alerting is about notifying relevant personnel when a specific condition is met, indicating a potential or active problem.

  * **How it works:**
      * Monitoring systems (like Prometheus) collect metrics.
      * Alerting rules are defined (e.g., "if `http_server_requests_total` for status `5xx` goes above 10 per minute").
      * When a rule fires, an alert is sent to an **Alert Manager**.
      * The Alert Manager handles deduplication, grouping, and routing alerts to appropriate channels (email, Slack, PagerDuty, SMS).
  * **Tools:**
      * **Prometheus Alertmanager:** Integrates directly with Prometheus.
      * **PagerDuty, Opsgenie, VictorOps:** Dedicated incident management platforms that integrate with various monitoring tools.
  * **Common Alert Scenarios:**
      * **Error Rate Thresholds:** 5xx error rate \> 5%.
      * **Latency Thresholds:** P99 request latency \> 1 second.
      * **Resource Utilization:** CPU \> 80% for 5 minutes, Disk space \< 10%.
      * **Service Unavailability:** Endpoint returning 4xx/5xx errors consistently or not reachable.
      * **Queue Lengths:** Celery queue growing rapidly.
      * **Failed Background Tasks:** A specific Celery task failing repeatedly.

You would define alerting rules in `prometheus.yml` or a separate `alert.rules` file that Prometheus loads, and configure Alertmanager to send notifications.

-----

#### Your Turn to Practice:

1.  **Implement Structured Logging in Your Task Manager API:**

      * Create a `logging_config.py` file with the `setup_logging` function and a `RequestIdFilter`.
      * Integrate `setup_logging()` into your `main.py`'s startup event.
      * Modify a few key endpoints (e.g., `register_user`, `create_task`) to use `logger.info`, `logger.warning`, `logger.error` with `extra` dictionaries for contextual information like `request_id` and `user_id`.
      * Observe the JSON output in your FastAPI Docker container logs.

2.  **Integrate Prometheus Metrics:**

      * Add `fastapi-prometheus` to your `requirements.txt`.
      * Update `main.py` to use `Instrumentator().instrument(app).expose(app)`.
      * Modify your `docker-compose.yml` to include `prometheus` and `grafana` services, and mount the `prometheus.yml` config.
      * Create the `prometheus/prometheus.yml` file with your `fastapi_app` target.
      * Run `docker compose up --build -d`.
      * Access Prometheus and Grafana UIs. Explore the metrics exposed by your FastAPI app (`http_requests_total`, `http_request_duration_seconds_bucket`, etc.) in Prometheus's "Graph" tab, and try to build a basic dashboard in Grafana.

This step will provide you with crucial visibility into your application's behavior, which is non-negotiable for production environments.

When you're ready, we'll move on to **Phase 4, Step 3: Cloud Deployment (AWS/Azure/GCP Basics)**, where we'll discuss how to get your containerized application running in the cloud.
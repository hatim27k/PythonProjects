Alright, we've built a robust API with authentication, authorization, and background tasks. Now, let's prepare it for the real world\! We're moving to **Phase 3, Step 7: Deployment & Containerization (Docker)**.

This phase will teach you how to package your application and its dependencies into a consistent and portable environment using Docker, which is the industry standard for deploying modern backend services.

-----

### Phase 3, Step 7: Deployment & Containerization (Docker)

#### 1\. Importance of Deployment & Containerization

  * **Consistency (Works on My Machine\!):** Eliminates the "it works on my machine" problem. Docker ensures that your application runs the same way in development, testing, and production environments, regardless of the underlying OS.
  * **Isolation:** Containers encapsulate your application and its dependencies (libraries, configuration, etc.), isolating them from the host system and other containers. This prevents conflicts and makes managing dependencies easier.
  * **Portability:** A Docker image can be run on any system with Docker installed (Linux, Windows, macOS, cloud VMs), making deployment across different environments seamless.
  * **Scalability:** Docker makes it easy to scale your application by spinning up multiple instances of your containerized services.
  * **Efficiency:** Containers are lightweight compared to traditional virtual machines (VMs) because they share the host OS kernel, leading to faster startup times and lower resource consumption.
  * **Simplified DevOps:** Facilitates CI/CD pipelines, allowing for automated building, testing, and deployment.

#### 2\. Introduction to Docker

**a. What is Docker?**
Docker is a platform that uses OS-level virtualization to deliver software in packages called containers.

  * **Containers vs. VMs:**

      * **Virtual Machines (VMs):** Each VM includes a full guest operating system on top of the host OS and hypervisor. They are heavy, slow to start, and consume more resources.
      * **Containers:** Share the host OS kernel. They only package the application, its dependencies, and a lightweight runtime environment. They are much lighter, faster to start, and consume fewer resources.

  * **Key Docker Components:**

      * **Dockerfile:** A text file containing instructions for building a Docker image. It's like a recipe.
      * **Image:** A lightweight, stand-alone, executable package that includes everything needed to run a piece of softwareâ€”code, a runtime, libraries, environment variables, and config files. Images are read-only templates.
      * **Container:** A runnable instance of an image. You can create, start, stop, move, or delete a container.

**b. Dockerfile Basics:**

A `Dockerfile` is a script that automates the steps to create a Docker image.

```dockerfile
# Start from a base image (e.g., Python slim for smaller size)
FROM python:3.11-slim-buster

# Set the working directory inside the container
WORKDIR /app

# Copy the requirements file into the container
COPY requirements.txt .

# Install Python dependencies
# --no-cache-dir: Don't store cache for pip, keeps image size down
# --upgrade pip: Ensure pip is up-to-date
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code into the container
COPY . .

# Expose the port your FastAPI application listens on
EXPOSE 8000

# Command to run the application when the container starts
# This will be overridden by docker-compose for specific services
# CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**c. Docker Compose:**

Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file (`docker-compose.yml`) to configure your application's services, networks, and volumes. Then, with a single command, you can create and start all the services from your configuration.

This is ideal for our setup:

  * One service for the FastAPI app.
  * One service for the Celery worker.
  * One service for PostgreSQL.
  * One service for Redis.

#### 3\. Building a Dockerized FastAPI + Celery + Redis + PostgreSQL App

We'll convert our previous setup to use PostgreSQL (a more robust production database than SQLite) and containerize everything.

**a. Prerequisites:**

  * **Docker Desktop (macOS/Windows) or Docker Engine (Linux):** Install Docker on your development machine. This provides the `docker` and `docker compose` commands.

**b. Database Choice: PostgreSQL**

Update your `database.py` to connect to PostgreSQL.

```python
# my_fastapi_app/database.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
import os

# Get database URL from environment variable
# Use os.getenv() for production, direct string for quick local testing (but not recommended)
# Example: postgresql://user:password@host:port/dbname
SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://user:password@db:5432/fastapi_db")

engine = create_engine(SQLALCHEMY_DATABASE_URL) # No check_same_thread needed for PostgreSQL

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()
```

  * **Note the `db:5432`:** `db` will be the hostname of our PostgreSQL service inside the Docker Compose network. `5432` is the default PostgreSQL port.
  * **Environment Variables:** We'll pass the `DATABASE_URL` via `docker-compose.yml`.

**c. `requirements.txt` (Ensure all dependencies are listed):**

Make sure you have all these in your `requirements.txt` in the root of your project:

```
fastapi==0.111.0
uvicorn==0.30.1
sqlalchemy==2.0.30
"python-jose[cryptography]==3.3.0"
"passlib[bcrypt]==1.7.4"
celery==5.4.0
redis==5.0.1
psycopg2-binary==2.9.9 # New: PostgreSQL database adapter
```

*Run `pip freeze > requirements.txt` to generate an up-to-date file.*

**d. Dockerfile for FastAPI App (`Dockerfile.app`):**

Create this file in your project root.

```dockerfile
# my_fastapi_app/Dockerfile.app
# Use a Python base image
FROM python:3.11-slim-buster

# Set working directory inside the container
WORKDIR /app

# Copy dependency files first to leverage Docker's build cache
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy the entire application code
COPY . .

# Expose the port the FastAPI application will run on
EXPOSE 8000

# Command to run the FastAPI application using Uvicorn
# This will be overridden by docker-compose for the 'app' service
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**e. Dockerfile for Celery Worker (`Dockerfile.worker`):**

This can be almost identical to the app's Dockerfile, as it needs the same Python environment.

```dockerfile
# my_fastapi_app/Dockerfile.worker
FROM python:3.11-slim-buster

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

COPY . .

# Command to run the Celery worker
# This will be overridden by docker-compose for the 'worker' service
CMD ["celery", "-A", "celery_worker", "worker", "--loglevel=info"]
```

**f. `docker-compose.yml` (The Orchestrator):**

This file defines all your services and how they interact. Place it in the root of your `my_fastapi_app/` directory.

```yaml
# my_fastapi_app/docker-compose.yml
version: '3.8' # Specify Docker Compose file format version

services:
  # FastAPI Application Service
  app:
    build:
      context: . # Build context is the current directory
      dockerfile: Dockerfile.app # Use the specific Dockerfile for the app
    ports:
      - "8000:8000" # Map host port 8000 to container port 8000
    volumes:
      - .:/app # Mount the current directory into /app inside the container (for live reload in dev)
    environment:
      # Database connection string for the app
      - DATABASE_URL=postgresql://user:password@db:5432/fastapi_db
      # Celery broker URL for the app to send tasks
      - CELERY_BROKER_URL=redis://redis:6379/0
      # Celery backend URL for results
      - CELERY_BACKEND_URL=redis://redis:6379/1
      # JWT Secret Key (VERY IMPORTANT: CHANGE FOR PRODUCTION!)
      - SECRET_KEY=your-super-secret-key-from-env # Match auth.py
    depends_on:
      - db # App service depends on the db service being up
      - redis # App service depends on redis being up
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload # Override CMD from Dockerfile for dev reload

  # Celery Worker Service
  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker # Use the specific Dockerfile for the worker
    volumes:
      - .:/app
    environment:
      # Database connection string for the worker (if tasks interact with DB)
      - DATABASE_URL=postgresql://user:password@db:5432/fastapi_db
      # Celery broker URL for the worker to receive tasks
      - CELERY_BROKER_URL=redis://redis:6379/0
      # Celery backend URL for results
      - CELERY_BACKEND_URL=redis://redis:6379/1
      # JWT Secret Key (if worker needs to decode JWTs or interact with protected services)
      - SECRET_KEY=your-super-secret-key-from-env # Match auth.py
    depends_on:
      - db
      - redis
      - app # Worker should also depend on app to ensure code is present
    command: celery -A celery_worker worker --loglevel=info # Override CMD from Dockerfile

  # PostgreSQL Database Service
  db:
    image: postgres:15-alpine # Use a lightweight PostgreSQL image
    volumes:
      - postgres_data:/var/lib/postgresql/data/ # Persist database data
    environment:
      # Environment variables for PostgreSQL setup
      - POSTGRES_DB=fastapi_db
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432" # Optional: Map DB port for external access (e.g., via DB client)

  # Redis Message Broker Service
  redis:
    image: redis/redis-stack-server:latest # Or just redis:latest for basic Redis
    ports:
      - "6379:6379" # Optional: Map Redis port for external access (e.g., via RedisInsight)
    volumes:
      - redis_data:/data # Persist Redis data

volumes:
  # Define named volumes for persistent data
  postgres_data:
  redis_data:
```

**g. Update `celery_worker.py` (to read broker/backend from env vars):**

```python
# celery_worker.py
from celery import Celery
import time
import os

# Get broker and backend URLs from environment variables
CELERY_BROKER_URL = os.getenv("CELERY_BROKER_URL", "redis://localhost:6379/0")
CELERY_BACKEND_URL = os.getenv("CELERY_BACKEND_URL", "redis://localhost:6379/1")

celery_app = Celery(
    'my_app',
    broker=CELERY_BROKER_URL,
    backend=CELERY_BACKEND_URL
)

# ... (your task definitions: send_welcome_email, generate_report, long_running_calculation) ...
```

**h. Update `auth.py` (to read SECRET\_KEY from env var):**

```python
# auth.py
from datetime import datetime, timedelta, timezone
from typing import Annotated
import os # Import os

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from passlib.context import CryptContext

# --- Configuration ---
SECRET_KEY = os.getenv("SECRET_KEY", "your-fallback-secret-key-for-dev") # Read from environment variable
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

# ... (rest of auth.py code) ...
```

**i. Database Migrations with Docker Compose:**

Since we're using PostgreSQL, we need to ensure our database schema is created/updated. `Base.metadata.create_all(bind=engine)` is fine for initial setup and development. For production, you would typically use a dedicated migration tool like **Alembic**.

**Running Initial Schema Creation with Docker Compose:**

You can create a temporary service in `docker-compose.yml` or run it as a one-off command.

**Option 1: One-off command (recommended for development)**

1.  First, ensure your `db` service is up: `docker compose up -d db`
2.  Then, run the schema creation command:
    ```bash
    docker compose run --rm app python -c "from database import Base, engine; Base.metadata.create_all(bind=engine); print('Database tables created via Docker Compose!')"
    ```
      * `docker compose run`: Runs a one-off command in a new container.
      * `--rm`: Removes the container after the command exits.
      * `app`: Use the `app` service's image and environment.
      * `python -c "..."`: The Python command to execute your schema creation.

**Option 2: As part of `app` service startup (simpler for small projects, but can lead to race conditions if DB isn't ready)**
You could add `command: bash -c "python -c 'from database import Base, engine; Base.metadata.create_all(bind=engine)' && uvicorn main:app --host 0.0.0.0 --port 8000 --reload"` to your `app` service, but this isn't robust.

**For Production:** Learn **Alembic**. It provides version-controlled database migrations, allowing you to gracefully evolve your schema.

#### 4\. Running the Stack

1.  **Open your terminal** in the `my_fastapi_app/` directory (where `docker-compose.yml` is).
2.  **Build images and start containers:**
    ```bash
    docker compose up --build -d
    ```
      * `--build`: Forces Docker to rebuild the images from the Dockerfiles (useful when you make code changes).
      * `-d`: Runs the containers in detached mode (in the background).
3.  **Check container status:**
    ```bash
    docker compose ps
    ```
    You should see `app`, `worker`, `db`, `redis` services running.
4.  **View logs (optional):**
    ```bash
    docker compose logs -f
    # Or for a specific service: docker compose logs -f app
    ```
5.  **Run initial database schema creation (important\!):**
    ```bash
    docker compose run --rm app python -c "from database import Base, engine; Base.metadata.create_all(bind=engine); print('Database tables created!')"
    ```
    You should see "Database tables created\!" in the output.

#### 5\. Testing the Dockerized App

Now, your entire application stack is running inside Docker containers.

  * Your FastAPI app is accessible at `http://localhost:8000`.
  * The Celery worker is listening to Redis.
  * PostgreSQL and Redis are running internally within the Docker network.

Test all your API endpoints (registration, login, protected routes, task dispatch, task status) using Postman or `curl`, just as you did before. Everything should work seamlessly\!

**To stop and remove containers:**

```bash
docker compose down
# To remove volumes (e.g., if you want a fresh DB next time):
# docker compose down --volumes
```

-----

#### Your Turn to Practice:

1.  **Containerize Your Task Manager API:**
      * Take your fully-featured `fastapi_auth_tasks` project (with FastAPI, SQLAlchemy, JWT, Celery).
      * **Modify `database.py`:** Switch from SQLite to PostgreSQL. Update the `SQLALCHEMY_DATABASE_URL` to use an environment variable and connect to the `db` service.
      * **Create `Dockerfile.app` and `Dockerfile.worker`:** Based on the examples provided.
      * **Update `requirements.txt`:** Add `psycopg2-binary`.
      * **Create `docker-compose.yml`:** Define `app`, `worker`, `db` (PostgreSQL), and `redis` services.
          * Ensure environment variables (`DATABASE_URL`, `CELERY_BROKER_URL`, `CELERY_BACKEND_URL`, `SECRET_KEY`) are correctly passed to `app` and `worker`.
          * Set up volumes for persistent data (`postgres_data`, `redis_data`).
          * Ensure `depends_on` are correctly set.
      * **Adapt `celery_worker.py` and `auth.py`:** To read their respective URLs/keys from environment variables using `os.getenv()`.
      * **Build and run the entire stack:** `docker compose up --build -d`
      * **Perform initial database schema creation.**
      * **Test all API endpoints and background tasks.**

This is a comprehensive and crucial step. Being able to containerize your applications is a fundamental skill for modern backend development and cloud deployment.

-----

**Congratulations\! You've completed Phase 3: Web Frameworks & APIs\!** This phase has equipped you with the practical skills to build robust, secure, and scalable backend services using FastAPI, SQLAlchemy, JWT, Celery, and Docker.

Are you ready to move on to **Phase 4: Advanced Topics & Cloud Deployment**? This will cover more advanced patterns, deployment to cloud providers, monitoring, and continuous integration/delivery.